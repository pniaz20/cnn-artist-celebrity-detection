{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "homedir = \"../\"\n",
    "datadir = \"../data/facescrub/\"\n",
    "sys.path.append(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_file = os.path.join(datadir, \"new_subset_actors.txt\")\n",
    "actresses_file = os.path.join(datadir, \"new_subset_actresses.txt\")\n",
    "target_folder = os.path.join(datadir, \"uncropped/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pouya/anaconda3/envs/comp541/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: URLopener style of invoking requests is deprecated. Use newer urlopen functions/methods\n",
      "  if sys.path[0] == \"\":\n"
     ]
    }
   ],
   "source": [
    "# Some constants\n",
    "img_size = 28\n",
    "input_size = img_size * img_size * 3\n",
    "trainsize = 70\n",
    "ready_data = os.path.join(datadir, \"data_size_28_training_70.parquet\")\n",
    "\n",
    "\n",
    "# Construct master dataset\n",
    "master_dataset = FaceScrubDataset(\n",
    "    actors_file=actors_file, actresses_file=actresses_file, target_image_size=img_size, \n",
    "    target_folder=target_folder, transform=None, train_size=trainsize, get_data_at_first_call=False,\n",
    "    delete_image_after_download=True, verbose=False)\n",
    "output_size = len(master_dataset.artists_list)\n",
    "master_dataset.load_data(ready_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing data with 224 pixels rather than 28.\n",
    "# Dataset object is the same. We will only change its target_image_size property.\n",
    "# We do not need to download and save all the images, because our dataset is alrewady processed, \n",
    "# and we know which URLs are available to use. We can more easily use direct indexing in pytorch dataset.\n",
    "\n",
    "# Setting some parameters to avoid direct indexing from arrays in the datasset\n",
    "master_dataset.target_image_size = 224\n",
    "master_dataset.data_is_processed = False\n",
    "master_dataset.get_data_at_first_call = False\n",
    "master_dataset.images = []\n",
    "master_dataset.labels = []\n",
    "master_dataset.df = master_dataset.df.iloc[:,:13]\n",
    "# We will not touch the 'labels' property because that one will not change.\n",
    "# We will not touch the 'df' property because that one will not change.\n",
    "\n",
    "# Since we will use live indexing, we will have to use num_workers=0 \n",
    "# to avoid data race and other multiprocessing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|█████████████████████████████████████████████████| 2650/2650 [46:54<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4963, 224, 224, 3)\n",
      "(4963,)\n",
      "(4963, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "master_dataset.process_all_data()\n",
    "print(master_dataset.images.shape)\n",
    "print(master_dataset.labels.shape)\n",
    "print(master_dataset.df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pouya/anaconda3/envs/comp541/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "np.savez(make_path(os.path.join(datadir, \"data_size_224_training_70\", \"data.npz\")), \n",
    "         images=master_dataset.images, labels=master_dataset.labels, \n",
    "         artists=np.array(master_dataset.artists_list, dtype=str))\n",
    "master_dataset.df.to_parquet(make_path(os.path.join(datadir, \"data_size_224_training_70\", \"info.parquet\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('comp541')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1021decb3be9f7575113891cc9ff0afbd89115dd797879e64fdb457aa9af662d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
